{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2e2a742",
   "metadata": {},
   "source": [
    "# Building the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de01bc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee15e3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_arxiv(query: str, subject: str = None, topic: str = None, max_results: int = 20):\n",
    "    \"\"\"\n",
    "    Search arXiv for papers.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query string\n",
    "        subject: Subject area (e.g., 'Artificial Intelligence')\n",
    "        topic: Topic within subject (e.g., 'Healthcare', 'NLP')\n",
    "        max_results: Maximum number of results to return\n",
    "    \"\"\"\n",
    "    # Build enhanced query with subject/topic if provided\n",
    "    full_query = query\n",
    "    if subject:\n",
    "        full_query = f\"{subject} {full_query}\"\n",
    "    if topic:\n",
    "        full_query = f\"{topic} {full_query}\"\n",
    "    \n",
    "    search = arxiv.Search(\n",
    "        query=full_query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance,\n",
    "    )\n",
    "    results = []\n",
    "    for result in search.results():\n",
    "        results.append({\n",
    "            \"source\": \"arxiv\",\n",
    "            \"id\": result.entry_id,\n",
    "            \"title\": result.title,\n",
    "            \"abstract\": result.summary,\n",
    "            \"year\": result.published.year,\n",
    "            \"venue\": \"arXiv\",\n",
    "            \"authors\": [a.name for a in result.authors],\n",
    "            \"citations\": None,\n",
    "            \"pdf_url\": result.pdf_url,\n",
    "            \"landing_url\": result.entry_id,\n",
    "            \"subject\": subject,\n",
    "            \"topic\": topic,\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f872bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Max filename length - reduced to ensure total path stays under Windows 260 char limit\n",
    "# Path structure: base (~150) + /subject (~50) + /topic (~50) + /filename + .pdf\n",
    "# So filename should be ~100 max to stay safe\n",
    "MAX_FILENAME_LENGTH = 100\n",
    "\n",
    "# RAG SETUP paths (relative to Tools Server/RAG SETUP)\n",
    "RAG_SETUP_PATH = Path(__file__).resolve().parent.parent.parent / \"Tools Server\" / \"RAG SETUP\" if \"__file__\" in dir() else Path(r\"c:\\Users\\User\\Desktop\\llms\\Project\\Research Assistant Multi Agent System\\Tools Server\\RAG SETUP\")\n",
    "PAPERS_PATH = RAG_SETUP_PATH / \"Papers\"\n",
    "VECTORDB_PATH = RAG_SETUP_PATH / \"VectorDB\"\n",
    "\n",
    "\n",
    "def sanitize_filename(name: str, max_length: int = MAX_FILENAME_LENGTH) -> str:\n",
    "    \"\"\"Remove invalid characters from filename and limit length.\"\"\"\n",
    "    # Remove characters not allowed in Windows filenames: \\ / : * ? \" < > |\n",
    "    sanitized = re.sub(r'[\\\\/:*?\"<>|]', '', name)\n",
    "    # Replace multiple spaces with single space\n",
    "    sanitized = re.sub(r'\\s+', ' ', sanitized)\n",
    "    # Trim and limit length\n",
    "    return sanitized.strip()[:max_length]\n",
    "\n",
    "\n",
    "def shorten_title(title: str, max_length: int = MAX_FILENAME_LENGTH) -> str:\n",
    "    \"\"\"\n",
    "    Shorten a paper title intelligently.\n",
    "    - If ≤ max_length: return as-is\n",
    "    - If > max_length and has ':': use part before ':'\n",
    "    - Otherwise: crop at max_length\n",
    "    \"\"\"\n",
    "    if len(title) <= max_length:\n",
    "        return title\n",
    "    \n",
    "    # Try to use the main title (before the colon)\n",
    "    if ':' in title:\n",
    "        main_title = title.split(':')[0].strip()\n",
    "        if len(main_title) <= max_length and len(main_title) > 10:  # Ensure it's meaningful\n",
    "            return main_title\n",
    "    \n",
    "    # Fallback: crop at max_length\n",
    "    return title[:max_length].strip()\n",
    "\n",
    "\n",
    "def _extract_paper_metadata(file_path: Path, base_path: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Extract metadata from a paper's file path.\n",
    "    Expected structure: base_path/Subject/Topic/title - year.pdf\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        \"subject\": \"Unknown\",\n",
    "        \"topic\": \"Unknown\", \n",
    "        \"paper_title\": file_path.stem,\n",
    "        \"year\": None,\n",
    "        \"file_name\": file_path.name,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Get relative path parts\n",
    "        rel_path = file_path.relative_to(base_path)\n",
    "        parts = rel_path.parts\n",
    "        \n",
    "        if len(parts) >= 3:\n",
    "            metadata[\"subject\"] = parts[0]\n",
    "            metadata[\"topic\"] = parts[1]\n",
    "        elif len(parts) == 2:\n",
    "            metadata[\"subject\"] = parts[0]\n",
    "            \n",
    "        # Extract year from filename: \"title - year.pdf\"\n",
    "        stem = file_path.stem\n",
    "        if \" - \" in stem:\n",
    "            title_part, year_part = stem.rsplit(\" - \", 1)\n",
    "            metadata[\"paper_title\"] = title_part.strip()\n",
    "            try:\n",
    "                metadata[\"year\"] = int(year_part.strip())\n",
    "            except ValueError:\n",
    "                pass\n",
    "                \n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "def _add_to_vectordb(file_path: Path, papers_base_path: Path = PAPERS_PATH, vectordb_path: Path = VECTORDB_PATH) -> bool:\n",
    "    \"\"\"\n",
    "    Add a downloaded PDF to the vector database.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the downloaded PDF\n",
    "        papers_base_path: Base path of the Papers directory\n",
    "        vectordb_path: Path to the Chroma vector database\n",
    "        \n",
    "    Returns:\n",
    "        True if successfully added, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the PDF\n",
    "        loader = PyMuPDFLoader(str(file_path))\n",
    "        raw_docs = loader.load()\n",
    "        \n",
    "        # Extract and add metadata\n",
    "        paper_metadata = _extract_paper_metadata(file_path, papers_base_path)\n",
    "        for doc in raw_docs:\n",
    "            doc.metadata.update(paper_metadata)\n",
    "            doc.metadata.update({\n",
    "                \"doc_id\": file_path.stem,\n",
    "                \"relpath\": str(file_path.relative_to(papers_base_path)) if papers_base_path in file_path.parents or file_path.parent == papers_base_path else file_path.name\n",
    "            })\n",
    "        \n",
    "        # Split into chunks\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=250)\n",
    "        split_docs = splitter.split_documents(raw_docs)\n",
    "        \n",
    "        # Load existing vectordb or create new one\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        \n",
    "        if (vectordb_path / \"chroma.sqlite3\").exists():\n",
    "            # Add to existing vectordb\n",
    "            vectordb = Chroma(\n",
    "                collection_name=\"research_papers\",\n",
    "                embedding_function=embeddings,\n",
    "                persist_directory=str(vectordb_path),\n",
    "            )\n",
    "            vectordb.add_documents(split_docs)\n",
    "        else:\n",
    "            # Create new vectordb\n",
    "            vectordb_path.mkdir(parents=True, exist_ok=True)\n",
    "            Chroma.from_documents(\n",
    "                documents=split_docs,\n",
    "                embedding=embeddings,\n",
    "                collection_name=\"research_papers\",\n",
    "                persist_directory=str(vectordb_path),\n",
    "            )\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to add document to vectordb: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def download_pdf(paper: dict, save_dir: str = None, file_name: str = None, add_to_vectordb: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Download PDF for a paper to: Papers/subject/topic/filename.pdf\n",
    "    and optionally add it to the vector database.\n",
    "    \n",
    "    Args:\n",
    "        paper: Paper dict with pdf_url, title, subject, topic, year\n",
    "        save_dir: Base directory for downloads (defaults to RAG Papers folder)\n",
    "        file_name: Optional custom filename (agent can provide a summarized name)\n",
    "        add_to_vectordb: Whether to automatically add the paper to the vector DB\n",
    "    \n",
    "    Returns:\n",
    "        Path to downloaded file (and status of vectordb addition)\n",
    "    \"\"\"\n",
    "    pdf_url = paper.get(\"pdf_url\")\n",
    "    if not pdf_url:\n",
    "        raise ValueError(\"No PDF URL available for this paper.\")\n",
    "    \n",
    "    # Default save_dir to the RAG Papers folder\n",
    "    if save_dir is None:\n",
    "        save_dir = PAPERS_PATH\n",
    "    else:\n",
    "        save_dir = Path(save_dir)\n",
    "    \n",
    "    # Build path: save_dir/subject/topic/\n",
    "    subject = paper.get(\"subject\") or \"General\"\n",
    "    topic = paper.get(\"topic\") or \"Uncategorized\"\n",
    "    \n",
    "    # Sanitize directory names\n",
    "    subject_dir = sanitize_filename(subject, max_length=50)\n",
    "    topic_dir = sanitize_filename(topic, max_length=50)\n",
    "    \n",
    "    # Use pathlib for cross-platform path handling\n",
    "    full_dir = Path(save_dir) / subject_dir / topic_dir\n",
    "    full_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Determine filename: use provided name, or shorten the title\n",
    "    if file_name:\n",
    "        base_name = sanitize_filename(file_name)\n",
    "    else:\n",
    "        shortened = shorten_title(paper.get(\"title\", \"paper\"))\n",
    "        base_name = sanitize_filename(shortened)\n",
    "    \n",
    "    # Add year and .pdf extension\n",
    "    year = paper.get(\"year\", \"\")\n",
    "    filename = f\"{base_name} - {year}.pdf\" if year else f\"{base_name}.pdf\"\n",
    "    \n",
    "    file_path = full_dir / filename\n",
    "\n",
    "    resp = requests.get(pdf_url, timeout=60)\n",
    "    resp.raise_for_status()\n",
    "    file_path.write_bytes(resp.content)\n",
    "    \n",
    "    # Add to vector database if requested\n",
    "    result_msg = f\"Downloaded: {file_path}\"\n",
    "    if add_to_vectordb:\n",
    "        vectordb_added = _add_to_vectordb(file_path, papers_base_path=Path(save_dir))\n",
    "        if vectordb_added:\n",
    "            result_msg += \" | Added to vector database ✓\"\n",
    "        else:\n",
    "            result_msg += \" | Failed to add to vector database\"\n",
    "\n",
    "    return result_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa81a01",
   "metadata": {},
   "source": [
    "# Trying out the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a78cb0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching arXiv for: medical diagnosis deep learning\n",
      "Subject: Artificial Intelligence | Topic: Healthcare\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_28248\\1971025978.py:24: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 papers\n",
      "\n",
      "1. Medical Knowledge-Guided Deep Curriculum Learning for Elbow Fracture Diagnosis from X-Ray Images\n",
      "   Year: 2021 | Authors: Jun Luo, Gene Kitamura...\n",
      "\n",
      "2. A Review on Explainable Artificial Intelligence for Healthcare: Why, How, and When?\n",
      "   Year: 2023 | Authors: Subrato Bharati, M. Rubaiyat Hossain Mondal...\n",
      "\n",
      "3. The Artificial Scientist: Logicist, Emergentist, and Universalist Approaches to Artificial General Intelligence\n",
      "   Year: 2021 | Authors: Michael Timothy Bennett, Yoshihiro Maruyama\n",
      "\n",
      "4. Privacy-preserving machine learning for healthcare: open challenges and future perspectives\n",
      "   Year: 2023 | Authors: Alejandro Guerra-Manzanares, L. Julian Lechuga Lopez...\n",
      "\n",
      "5. Artificial Intelligence Framework for Simulating Clinical Decision-Making: A Markov Decision Process Approach\n",
      "   Year: 2013 | Authors: Casey C. Bennett, Kris Hauser\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test arXiv search with subject and topic\n",
    "query = \"medical diagnosis deep learning\"\n",
    "subject = \"Artificial Intelligence\"\n",
    "topic = \"Healthcare\"\n",
    "\n",
    "print(f\"Searching arXiv for: {query}\")\n",
    "print(f\"Subject: {subject} | Topic: {topic}\\n\")\n",
    "\n",
    "results = search_arxiv(query, subject=subject, topic=topic, max_results=5)\n",
    "print(f\"Found {len(results)} papers\\n\")\n",
    "\n",
    "for i, paper in enumerate(results, 1):\n",
    "    print(f\"{i}. {paper['title']}\")\n",
    "    print(f\"   Year: {paper['year']} | Authors: {', '.join(paper['authors'][:2])}{'...' if len(paper['authors']) > 2 else ''}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e8dcdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing shorten_title():\n",
      "\n",
      "Original (11 chars): Short Title\n",
      "Shortened (11 chars): Short Title\n",
      "\n",
      "Original (83 chars): A Review on Explainable Artificial Intelligence for Healthcare: Why, How, and When?\n",
      "Shortened (83 chars): A Review on Explainable Artificial Intelligence for Healthcare: Why, How, and When?\n",
      "\n",
      "Original (96 chars): Medical Knowledge-Guided Deep Curriculum Learning for Elbow Fracture Diagnosis from X-Ray Images\n",
      "Shortened (96 chars): Medical Knowledge-Guided Deep Curriculum Learning for Elbow Fracture Diagnosis from X-Ray Images\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the shorten_title function with examples\n",
    "print(\"Testing shorten_title():\\n\")\n",
    "\n",
    "test_titles = [\n",
    "    \"Short Title\",  # ≤ 80\n",
    "    \"A Review on Explainable Artificial Intelligence for Healthcare: Why, How, and When?\",  # Has :\n",
    "    \"Medical Knowledge-Guided Deep Curriculum Learning for Elbow Fracture Diagnosis from X-Ray Images\",  # No : but long\n",
    "]\n",
    "\n",
    "for title in test_titles:\n",
    "    shortened = shorten_title(title)\n",
    "    print(f\"Original ({len(title)} chars): {title}\")\n",
    "    print(f\"Shortened ({len(shortened)} chars): {shortened}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f448b4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Medical Knowledge-Guided Deep Curriculum Learning for Elbow ...\n",
      "Shortened: Medical Knowledge-Guided Deep Curriculum Learning for Elbow Fracture Diagnosis from X-Ray Images\n",
      "✗ Failed: [Errno 2] No such file or directory: 'test_downloads\\\\Artificial Intelligence\\\\Healthcare\\\\Medical Knowledge-Guided Deep Curriculum Learning for Elbow Fracture Diagnosis from X-Ray Images - 2021.pdf'\n",
      "\n",
      "Title: A Review on Explainable Artificial Intelligence for Healthca...\n",
      "Shortened: A Review on Explainable Artificial Intelligence for Healthcare: Why, How, and When?\n",
      "✗ Failed: [Errno 2] No such file or directory: 'test_downloads\\\\Artificial Intelligence\\\\Healthcare\\\\Medical Knowledge-Guided Deep Curriculum Learning for Elbow Fracture Diagnosis from X-Ray Images - 2021.pdf'\n",
      "\n",
      "Title: A Review on Explainable Artificial Intelligence for Healthca...\n",
      "Shortened: A Review on Explainable Artificial Intelligence for Healthcare: Why, How, and When?\n",
      "✓ Saved to: test_downloads\\Artificial Intelligence\\Healthcare\\A Review on Explainable Artificial Intelligence for Healthcare Why, How, and When - 2023.pdf\n",
      "\n",
      "✓ Saved to: test_downloads\\Artificial Intelligence\\Healthcare\\A Review on Explainable Artificial Intelligence for Healthcare Why, How, and When - 2023.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test downloading papers\n",
    "for paper in results[:2]:\n",
    "    print(f\"Title: {paper['title'][:60]}...\")\n",
    "    print(f\"Shortened: {shorten_title(paper['title'])}\")\n",
    "    try:\n",
    "        path = download_pdf(paper, save_dir=\"test_downloads\")\n",
    "        print(f\"✓ Saved to: {path}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a21c1b",
   "metadata": {},
   "source": [
    "# Building the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2596f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import List, Annotated, TypedDict\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, ToolMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, model, tools, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\",\n",
    "            self.exists_action,\n",
    "            {True: \"action\", False: END}\n",
    "        )\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile()\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        want_tools = isinstance(result, AIMessage) and bool(getattr(result, \"tool_calls\", None))\n",
    "        return  want_tools\n",
    "\n",
    "    async def call_openai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = await self.model.ainvoke(messages) # aynchronous invoke\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    async def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            if not t['name'] in self.tools:      # check for bad tool name from LLM\n",
    "                print(\"\\n ....bad tool name....\")\n",
    "                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n",
    "            else:\n",
    "                result = await self.tools[t['name']].ainvoke(t['args']) # aynchronous invoke\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229126b8",
   "metadata": {},
   "source": [
    "# Trying out the agents with its tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "989a791a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent initialized with tools: ['search_arxiv', 'download_pdf']\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import StructuredTool\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "# Define input schemas for the tools\n",
    "class SearchArxivInput(BaseModel):\n",
    "    query: str = Field(..., description=\"Search query for finding papers\")\n",
    "    subject: Optional[str] = Field(default=None, description=\"Subject area (e.g., 'Artificial Intelligence')\")\n",
    "    topic: Optional[str] = Field(default=None, description=\"Topic within subject (e.g., 'Healthcare', 'NLP')\")\n",
    "    max_results: int = Field(default=10, description=\"Maximum number of results to return\")\n",
    "\n",
    "class DownloadPdfInput(BaseModel):\n",
    "    pdf_url: str = Field(..., description=\"The PDF URL from the search results\")\n",
    "    title: str = Field(..., description=\"Title of the paper\")\n",
    "    year: Optional[int] = Field(default=None, description=\"Publication year\")\n",
    "    subject: Optional[str] = Field(default=None, description=\"Subject area for organizing the download\")\n",
    "    topic: Optional[str] = Field(default=None, description=\"Topic for organizing the download\")\n",
    "    add_to_vectordb: bool = Field(default=True, description=\"Whether to add the paper to the RAG vector database\")\n",
    "\n",
    "# Wrapper function that converts flat args to paper dict\n",
    "def download_pdf_wrapper(pdf_url: str, title: str, year: int = None, subject: str = None, topic: str = None, add_to_vectordb: bool = True) -> str:\n",
    "    paper = {\n",
    "        \"pdf_url\": pdf_url,\n",
    "        \"title\": title,\n",
    "        \"year\": year,\n",
    "        \"subject\": subject,\n",
    "        \"topic\": topic,\n",
    "    }\n",
    "    return download_pdf(paper, add_to_vectordb=add_to_vectordb)\n",
    "\n",
    "# Create structured tools\n",
    "search_tool = StructuredTool.from_function(\n",
    "    name=\"search_arxiv\",\n",
    "    description=\"Search arXiv for academic papers. Returns a list of papers with title, abstract, authors, year, pdf_url, subject, and topic.\",\n",
    "    func=search_arxiv,\n",
    "    args_schema=SearchArxivInput,\n",
    ")\n",
    "\n",
    "download_tool = StructuredTool.from_function(\n",
    "    name=\"download_pdf\",\n",
    "    description=\"Download a PDF paper and automatically add it to the RAG vector database. Use the pdf_url and title from search results. Saves to: Papers/subject/topic/title - year.pdf\",\n",
    "    func=download_pdf_wrapper,\n",
    "    args_schema=DownloadPdfInput,\n",
    ")\n",
    "\n",
    "tools = [search_tool, download_tool]\n",
    "\n",
    "# Initialize the agent\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "system_prompt = \"\"\"You are a research assistant that helps users find and download academic papers from arXiv.\n",
    "\n",
    "You have two tools:\n",
    "1. search_arxiv: Search for papers on arXiv by query, subject, and topic\n",
    "2. download_pdf: Download a paper's PDF using its pdf_url and title from search results. \n",
    "   The paper will be automatically added to the RAG vector database for future retrieval.\n",
    "\n",
    "When a user asks you to find papers:\n",
    "- Use search_arxiv with relevant query terms\n",
    "- Include subject and topic if the user specifies them\n",
    "\n",
    "When a user asks to download a paper:\n",
    "- Extract the pdf_url, title, year, subject, and topic from the search results\n",
    "- Pass these to download_pdf\n",
    "- The PDF will be saved to: Papers/subject/topic/title - year.pdf\n",
    "- The paper will automatically be indexed in the vector database\n",
    "\n",
    "Always summarize the results for the user.\"\"\"\n",
    "\n",
    "agent = Agent(llm, tools, system=system_prompt)\n",
    "print(\"Agent initialized with tools:\", [t.name for t in tools])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12ad4875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling: {'name': 'search_arxiv', 'args': {'query': 'deep learning medical image analysis', 'max_results': 3}, 'id': 'call_StDAOMTHI44phNpVtuaMbwfm', 'type': 'tool_call'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_28248\\1971025978.py:24: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Back to the model!\n",
      "Here are three papers related to deep learning for medical image analysis:\n",
      "\n",
      "1. **[A Survey on Active Learning and Human-in-the-Loop Deep Learning for Medical Image Analysis](http://arxiv.org/abs/1910.02923v2)** (2019)\n",
      "   - **Authors**: Samuel Budd, Emma C Robinson, Bernhard Kainz\n",
      "   - **Abstract**: This paper reviews the role of human involvement in deep learning systems for medical image analysis. It discusses techniques for active learning, interaction with model outputs, practical considerations for deployment, and future research directions.\n",
      "   - **[Download PDF](https://arxiv.org/pdf/1910.02923v2)**\n",
      "\n",
      "2. **[TransMorph: Transformer for unsupervised medical image registration](http://arxiv.org/abs/2111.10480v6)** (2021)\n",
      "   - **Authors**: Junyu Chen, Eric C. Frey, Yufan He, William P. Segars, Ye Li, Yong Du\n",
      "   - **Abstract**: This paper presents TransMorph, a hybrid Transformer-ConvNet model for volumetric medical image registration. It evaluates the model's performance against existing methods and demonstrates significant improvements in registration tasks.\n",
      "   - **[Download PDF](https://arxiv.org/pdf/2111.10480v6)**\n",
      "\n",
      "3. **[Cross-dimensional transfer learning in medical image segmentation with deep learning](http://arxiv.org/abs/2307.15872v1)** (2023)\n",
      "   - **Authors**: Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem, Pierre-Henri Conze\n",
      "   - **Abstract**: This paper introduces a method for transferring knowledge from 2D classification networks to 2D and 3D medical image segmentation tasks. It discusses novel architectures and presents results from various medical imaging benchmarks.\n",
      "   - **[Download PDF](https://arxiv.org/pdf/2307.15872v1)**\n",
      "\n",
      "If you would like to download any of these papers, please let me know!\n",
      "Here are three papers related to deep learning for medical image analysis:\n",
      "\n",
      "1. **[A Survey on Active Learning and Human-in-the-Loop Deep Learning for Medical Image Analysis](http://arxiv.org/abs/1910.02923v2)** (2019)\n",
      "   - **Authors**: Samuel Budd, Emma C Robinson, Bernhard Kainz\n",
      "   - **Abstract**: This paper reviews the role of human involvement in deep learning systems for medical image analysis. It discusses techniques for active learning, interaction with model outputs, practical considerations for deployment, and future research directions.\n",
      "   - **[Download PDF](https://arxiv.org/pdf/1910.02923v2)**\n",
      "\n",
      "2. **[TransMorph: Transformer for unsupervised medical image registration](http://arxiv.org/abs/2111.10480v6)** (2021)\n",
      "   - **Authors**: Junyu Chen, Eric C. Frey, Yufan He, William P. Segars, Ye Li, Yong Du\n",
      "   - **Abstract**: This paper presents TransMorph, a hybrid Transformer-ConvNet model for volumetric medical image registration. It evaluates the model's performance against existing methods and demonstrates significant improvements in registration tasks.\n",
      "   - **[Download PDF](https://arxiv.org/pdf/2111.10480v6)**\n",
      "\n",
      "3. **[Cross-dimensional transfer learning in medical image segmentation with deep learning](http://arxiv.org/abs/2307.15872v1)** (2023)\n",
      "   - **Authors**: Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem, Pierre-Henri Conze\n",
      "   - **Abstract**: This paper introduces a method for transferring knowledge from 2D classification networks to 2D and 3D medical image segmentation tasks. It discusses novel architectures and presents results from various medical imaging benchmarks.\n",
      "   - **[Download PDF](https://arxiv.org/pdf/2307.15872v1)**\n",
      "\n",
      "If you would like to download any of these papers, please let me know!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Test the agent\n",
    "async def test_agent():\n",
    "    query = \"Find me 3 papers about deep learning for medical image analysis\"\n",
    "    result = await agent.graph.ainvoke({\"messages\": [HumanMessage(content=query)]})\n",
    "    return result[\"messages\"][-1].content\n",
    "\n",
    "# Run the test\n",
    "import asyncio\n",
    "response = await test_agent()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cb896e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling: {'name': 'search_arxiv', 'args': {'query': 'transformer architecture', 'subject': 'Artificial Intelligence', 'topic': 'Deep Learning', 'max_results': 1}, 'id': 'call_o1vuSVJPR7J9BJkePjEhNuJR', 'type': 'tool_call'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_28248\\1971025978.py:24: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Back to the model!\n",
      "Calling: {'name': 'download_pdf', 'args': {'pdf_url': 'https://arxiv.org/pdf/2110.01831v1', 'title': 'The Artificial Scientist: Logicist, Emergentist, and Universalist Approaches to Artificial General Intelligence', 'year': 2021, 'subject': 'Artificial Intelligence', 'topic': 'Deep Learning'}, 'id': 'call_Viq24nnqM0stb1wZAmefw6V6', 'type': 'tool_call'}\n",
      "Calling: {'name': 'download_pdf', 'args': {'pdf_url': 'https://arxiv.org/pdf/2110.01831v1', 'title': 'The Artificial Scientist: Logicist, Emergentist, and Universalist Approaches to Artificial General Intelligence', 'year': 2021, 'subject': 'Artificial Intelligence', 'topic': 'Deep Learning'}, 'id': 'call_Viq24nnqM0stb1wZAmefw6V6', 'type': 'tool_call'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_28248\\1444543260.py:124: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Back to the model!\n",
      "I found a paper titled **\"The Artificial Scientist: Logicist, Emergentist, and Universalist Approaches to Artificial General Intelligence\"** by Michael Timothy Bennett and Yoshihiro Maruyama, published in 2021. \n",
      "\n",
      "The paper explores various approaches to artificial general intelligence (AGI) and argues for a unified or hybrid approach to constructing an Artificial Scientist.\n",
      "\n",
      "I have downloaded the paper, and it is saved as **\"The Artificial Scientist - 2021.pdf\"** in the directory for Artificial Intelligence and Deep Learning. The paper has also been added to the vector database for future retrieval.\n",
      "I found a paper titled **\"The Artificial Scientist: Logicist, Emergentist, and Universalist Approaches to Artificial General Intelligence\"** by Michael Timothy Bennett and Yoshihiro Maruyama, published in 2021. \n",
      "\n",
      "The paper explores various approaches to artificial general intelligence (AGI) and argues for a unified or hybrid approach to constructing an Artificial Scientist.\n",
      "\n",
      "I have downloaded the paper, and it is saved as **\"The Artificial Scientist - 2021.pdf\"** in the directory for Artificial Intelligence and Deep Learning. The paper has also been added to the vector database for future retrieval.\n"
     ]
    }
   ],
   "source": [
    "# Test downloading a paper - it should save to Papers folder and add to vectordb\n",
    "query = \"Search for a paper about transformer architecture and download the first result. Use subject 'Artificial Intelligence' and topic 'Deep Learning'\"\n",
    "result = await agent.graph.ainvoke({\"messages\": [HumanMessage(content=query)]})\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d9998fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 results:\n",
      "\n",
      "--- Result 1 ---\n",
      "Subject: Artificial Intelligence\n",
      "Topic: Deep Learning\n",
      "Paper Title: The Artificial Scientist\n",
      "Year: 2021\n",
      "Content preview: arXiv:2110.01831v1  [cs.AI]  5 Oct 2021\n",
      "The Artiﬁcial Scientist:\n",
      "Logicist, Emergentist, and Universalist\n",
      "Approaches to Artiﬁcial General Intelligence⋆\n",
      "Michael Timothy Bennett and Yoshihiro Maruyama\n",
      "Sc...\n",
      "\n",
      "--- Result 2 ---\n",
      "Subject: Artificial Intelligence\n",
      "Topic: Deep Learning\n",
      "Paper Title: The Artificial Scientist\n",
      "Year: 2021\n",
      "Content preview: ded to function within the conﬁnes of a speciﬁc environment, enacted through\n",
      "what an organism does and, ﬁnally, extending into that environment to store\n",
      "and retrieve information. All of this seems obv...\n",
      "\n",
      "--- Result 3 ---\n",
      "Subject: Artificial Intelligence\n",
      "Topic: Deep Learning\n",
      "Paper Title: The Artificial Scientist\n",
      "Year: 2021\n",
      "Content preview: The Artiﬁcial Scientist\n",
      "5\n",
      "theoretical agent, named AIXI, has been proven to perform such that there is\n",
      "no other agent which outperforms it in one environment that can also equal\n",
      "its performance in all...\n"
     ]
    }
   ],
   "source": [
    "# Verify the paper was indexed in the vectordb\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectordb = Chroma(\n",
    "    collection_name=\"research_papers\",\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    "    persist_directory=str(VECTORDB_PATH),\n",
    ")\n",
    "\n",
    "# Search for content from the newly downloaded paper\n",
    "results = vectordb.similarity_search(\"Artificial Scientist AGI approaches\", k=3)\n",
    "print(f\"Found {len(results)} results:\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n--- Result {i+1} ---\")\n",
    "    print(f\"Subject: {doc.metadata.get('subject')}\")\n",
    "    print(f\"Topic: {doc.metadata.get('topic')}\")\n",
    "    print(f\"Paper Title: {doc.metadata.get('paper_title')}\")\n",
    "    print(f\"Year: {doc.metadata.get('year')}\")\n",
    "    print(f\"Content preview: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a45ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers Path: c:\\Users\\User\\Desktop\\llms\\Project\\Research Assistant Multi Agent System\\Tools Server\\RAG SETUP\\Papers\n",
      "VectorDB Path: c:\\Users\\User\\Desktop\\llms\\Project\\Research Assistant Multi Agent System\\Tools Server\\RAG SETUP\\VectorDB\n",
      "Papers Path exists: True\n",
      "VectorDB Path exists: True\n",
      "\n",
      "Downloading test paper: Attention Is All You Need\n",
      "\n",
      "Result: Downloaded: c:\\Users\\User\\Desktop\\llms\\Project\\Research Assistant Multi Agent System\\Tools Server\\RAG SETUP\\Papers\\Artificial Intelligence\\Deep Learning\\Attention Is All You Need - 2017.pdf | Added to vector database ✓\n",
      "\n",
      "Result: Downloaded: c:\\Users\\User\\Desktop\\llms\\Project\\Research Assistant Multi Agent System\\Tools Server\\RAG SETUP\\Papers\\Artificial Intelligence\\Deep Learning\\Attention Is All You Need - 2017.pdf | Added to vector database ✓\n"
     ]
    }
   ],
   "source": [
    "# Direct test of the download function\n",
    "\n",
    "# Verify paths are correct\n",
    "print(f\"Papers Path: {PAPERS_PATH}\")\n",
    "print(f\"VectorDB Path: {VECTORDB_PATH}\")\n",
    "print(f\"Papers Path exists: {PAPERS_PATH.exists()}\")\n",
    "print(f\"VectorDB Path exists: {VECTORDB_PATH.exists()}\")\n",
    "\n",
    "# Test paper data (from a real arXiv paper)\n",
    "test_paper = {\n",
    "    \"pdf_url\": \"https://arxiv.org/pdf/1706.03762\",  # \"Attention Is All You Need\"\n",
    "    \"title\": \"Attention Is All You Need\",\n",
    "    \"year\": 2017,\n",
    "    \"subject\": \"Artificial Intelligence\",\n",
    "    \"topic\": \"Deep Learning\",\n",
    "}\n",
    "\n",
    "print(f\"\\nDownloading test paper: {test_paper['title']}\")\n",
    "result = download_pdf(test_paper, add_to_vectordb=True)\n",
    "print(f\"\\nResult: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
